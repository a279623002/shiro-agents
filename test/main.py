import dotenv
import os
from openai import OpenAI

dotenv.load_dotenv()

model_id = os.getenv("LLM_MODEL_ID")
api_key = os.getenv("LLM_API_KEY")
base_url = os.getenv("LLM_BASE_URL")    

llm = OpenAI(
    api_key=api_key,
    base_url=base_url,
)       

prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„ç›®çš„åœ°{destination},ç”Ÿæˆè¯¦ç»†çš„æ—…è¡Œè®¡åˆ’ã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›æ—…è¡Œè®¡åˆ’:
```json
{{
    "destinations": [
        {{
            "name": "ç›®çš„åœ°åç§°",
            "days": 1,
            "activities": [
                "æ´»åŠ¨1",
                "æ´»åŠ¨2",
                "æ´»åŠ¨3"
            ]
        }}
    ]
}}   
```
"""

# è·å–ç”¨æˆ·è¾“å…¥
user_input = input("è¯·è¾“å…¥æ‚¨çš„ç›®çš„åœ°: ")

# æ„å»ºå®Œæ•´çš„æç¤º
full_prompt = prompt.format(destination=user_input)

print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {model_id} æ¨¡å‹...")
try:
    response = llm.chat.completions.create(
        model=model_id,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸åŠ©æ‰‹"},
            {"role": "user", "content": full_prompt}
        ],
        temperature=0.7,
        max_tokens=1024,
        stream=True,
    )

    # å¤„ç†æµå¼å“åº”
    print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
    for chunk in response:
        content = chunk.choices[0].delta.content or ""
        if content:
            print(content, end="", flush=True)
    print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ

except Exception as e:
    print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
